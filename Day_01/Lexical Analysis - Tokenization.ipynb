{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Error loading avergaed_perceptron_tagger: Package\n",
      "[nltk_data]     'avergaed_perceptron_tagger' not found in index\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package indian to C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package indian is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') # Tokenization\n",
    "nltk.download('stopwords') # Stopwords Removal\n",
    "nltk.download('avergaed_perceptron_tagger') # POS (Part-Of-Speech) Tagging\n",
    "nltk.download('wordnet') # Wordnet database and lemmatization\n",
    "nltk.download('omw-1.4') # Stemming\n",
    "nltk.download('indian') # Indian Language POS Tagging\n",
    "nltk.download('maxent_ne_chunker') # chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'They told that their ages are 25 27 and 31 respectively'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['They', 'told', 'that', 'their', 'ages', 'are', '25', '27', 'and', '31', 'respectively']\n",
      "25\n",
      "27\n",
      "31\n",
      "Mean is:  27.666666666666668\n"
     ]
    }
   ],
   "source": [
    "# Find the average of ages mentioned in the above sentence\n",
    "\n",
    "sample = sent.split(sep=' ')\n",
    "print(sample)\n",
    "\n",
    "sum=0\n",
    "count=0\n",
    "\n",
    "for i in sample:\n",
    "    if i.isnumeric():\n",
    "        print(i) \n",
    "        sum = sum+int(i)\n",
    "        count = count+1\n",
    "\n",
    "if count>0:\n",
    "    print('Mean is: ',sum/count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.666666666666668"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Other Way\n",
    "import numpy as np\n",
    "\n",
    "# List Comprehension\n",
    "np.mean([int(word) for word in sent.split() if word.isdigit()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'Hello Friends! How are you? Welcome to Python Programming. My age is 25.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Friends!',\n",
       " 'How are you?',\n",
       " 'Welcome to Python Programming.',\n",
       " 'My age is 25.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Segmentation\n",
    "sent_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(sent_tokenize(sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'Python',\n",
       " 'Programming',\n",
       " '.',\n",
       " 'My',\n",
       " 'age',\n",
       " 'is',\n",
       " '25',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(word_tokenize(sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '?', '.', '.']\n",
      "Probability of punctuation in sentence:  0.23529411764705882\n"
     ]
    }
   ],
   "source": [
    "puncs = list(word for word in word_tokenize(sent) if (word.isalpha()==False | word.isnumeric()==False))\n",
    "print(puncs)\n",
    "print(\"Probability of punctuation in sentence: \", len(puncs)/len(word_tokenize(sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of punctuation in sentence:  0.23529411764705882\n"
     ]
    }
   ],
   "source": [
    "# Other way\n",
    "puncs = [word for word in word_tokenize(sent) if not word.isalnum()]\n",
    "print(\"Probability of punctuation in sentence: \", len(puncs)/len(word_tokenize(sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Character to ASCII\n",
    "ord('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getsizeof('##')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function getsizeof in module sys:\n",
      "\n",
      "getsizeof(...)\n",
      "    getsizeof(object [, default]) -> int\n",
      "    \n",
      "    Return the size of object in bytes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sys.getsizeof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char = 'abcd'\n",
    "sys.getsizeof(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char = 'a'\n",
    "sys.getsizeof(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x01'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ASCII(American Standard Code for Information Interchange) to Character; there is also ISCII\n",
    "chr(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "à¤·\n"
     ]
    }
   ],
   "source": [
    "char = '\\u0937'\n",
    "print(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "à¤·à¥€\n"
     ]
    }
   ],
   "source": [
    "char = '\\u0937\\u0940'\n",
    "print(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2359"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('à¤·')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'à¤µ'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hexadecimal\n",
    "chr(0x935)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof('à¤·')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'à¤ªà¥à¤°à¤¥à¤®à¥‡à¤¶ à¤•à¤¾à¤³à¥‡'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['à¤ªà¥à¤°à¤¥à¤®à¥‡à¤¶', 'à¤•à¤¾à¤³à¥‡']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name.startswith('à¤¥')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Praà¥à¤°à¤¥à¤®à¥‡à¤¶ à¤•à¤¾à¤³à¥‡'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name.replace('à¤ª','Pra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name.find('à¤³à¥‡')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "5\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "names = ['à¤ªà¥à¤°à¤¥à¤®à¥‡à¤¶' , 'à¤‰à¤¤à¥à¤¸à¤µ' , 'à¤—à¥Œà¤°à¤µ', 'à¤¶à¥à¤°à¥€à¤¨à¤¿à¤µà¤¾à¤¸' , 'à¤ªà¤¾à¤°à¥à¤¥' , 'à¤†à¤¦à¤¿à¤¤à¥à¤¯' , 'à¤…à¤®à¥‡à¤¯' ]\n",
    "\n",
    "for name in names:\n",
    "    if name.startswith('à¤ª'):\n",
    "        print(len(name))\n",
    "        \n",
    "        \n",
    "print(len('à¤¶à¥à¤°à¥€à¤¨à¤¿à¤µà¤¾à¤¸'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Friends! How are you?\\n', 'Welcome to \\tthe world of Python Programming.']\n"
     ]
    }
   ],
   "source": [
    "with open ('sample.txt') as file1:\n",
    "    a = file1.readlines()\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Space Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Friends! How are you?\n",
      "Welcome to \tthe world of Python Programming.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you?\\nWelcome',\n",
       " 'to',\n",
       " '\\tthe',\n",
       " 'world',\n",
       " 'of',\n",
       " 'Python',\n",
       " 'Programming.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('sample.txt')\n",
    "data = f.read()\n",
    "print(data)\n",
    "\n",
    "# SpaceTokenizer(): Tokenize a string using the space character as a delimiter, which is the same as ``s.split(' ')\n",
    "from nltk.tokenize import SpaceTokenizer\n",
    "\n",
    "# Creating the Object\n",
    "tk = SpaceTokenizer()\n",
    "\n",
    "# Tokenize the data - Tokenization\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tab Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Friends! How are you?\n",
      "Welcome to \tthe world of Python Programming.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hello Friends! How are you?\\nWelcome to ',\n",
       " 'the world of Python Programming.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('sample.txt')\n",
    "data = f.read()\n",
    "print(data)\n",
    "\n",
    "# TabTokenizer(): Tokenize a string use the tab character as a delimiter, the same as ``s.split('\\t')\n",
    "from nltk.tokenize import TabTokenizer\n",
    "\n",
    "# Creating the Object\n",
    "tk = TabTokenizer()\n",
    "\n",
    "# Tokenize the data - Tokenization\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Friends! How are you?\n",
      "Welcome to \tthe world of Python Programming.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hello Friends! How are you?',\n",
       " 'Welcome to \\tthe world of Python Programming.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('sample.txt')\n",
    "data = f.read()\n",
    "print(data)\n",
    "\n",
    "# LineTokenizer(): Tokenize a string into its lines, optionally discarding blank lines. This is similar to ``s.split('\\n')\n",
    "from nltk.tokenize import LineTokenizer\n",
    "\n",
    "# Creating the Object\n",
    "tk = LineTokenizer()\n",
    "\n",
    "# Tokenize the data - Tokenization\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whitespace Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Friends! How are you?\n",
      "Welcome to \tthe world of Python Programming.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'Python',\n",
       " 'Programming.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('sample.txt')\n",
    "data = f.read()\n",
    "print(data)\n",
    "\n",
    "# WhitespaceTokenizer(): Tokenize a string on whitespace (space, tab, newline). In general, users should use the string ``split()`` method instead.\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "# Creating the Object\n",
    "tk = WhitespaceTokenizer()\n",
    "\n",
    "# Tokenize the data - Tokenization\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordPunc Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Friends! How are you?\n",
      "Welcome to \tthe world of Python Programming.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'Python',\n",
       " 'Programming',\n",
       " '.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('sample.txt')\n",
    "data = f.read()\n",
    "print(data)\n",
    "\n",
    "# WordPunctTokenizer(): Tokenize a text into a sequence of alphabetic and non-alphabetic characters, using the regexp ``\\w+|[^\\w\\s]+\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "# Creating the Object\n",
    "tk = WordPunctTokenizer()\n",
    "\n",
    "# Tokenize the data - Tokenization\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MWE (Multi-Word Expression) Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Van Rossum',\n",
       " 'is',\n",
       " 'Python',\n",
       " 'Creator',\n",
       " ',',\n",
       " 'visiting',\n",
       " 'Pune',\n",
       " 'this',\n",
       " 'week',\n",
       " '.',\n",
       " 'The',\n",
       " 'development',\n",
       " 'community',\n",
       " 'is',\n",
       " 'very',\n",
       " 'eager',\n",
       " 'to',\n",
       " 'meet',\n",
       " 'Van Rossum']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1 = '''The Van Rossum is Python Creator, visiting Pune this week. The\n",
    "development community is very eager to meet Van Rossum'''\n",
    "\n",
    "# MWETokenizer(): A tokenizer that processes tokenized text and merges multi-word expressions into single tokens.\n",
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "# Creating the Object\n",
    "tk = MWETokenizer(separator=' ') # Default Seperator is '_'\n",
    "tk.add_mwe(('Van','Rossum'))\n",
    "\n",
    "# Tokenize the data - Tokenization\n",
    "tk.tokenize(word_tokenize(sent1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Van Rossum is Python CreatorðŸ˜‰, visiting Pune this week ðŸ˜. The\n",
      "development community is very eager to meet Van RossumðŸ˜€\n",
      "['The', 'Van', 'Rossum', 'is', 'Python', 'CreatorðŸ˜‰', ',', 'visiting', 'Pune', 'this', 'week', 'ðŸ˜', '.', 'The', 'development', 'community', 'is', 'very', 'eager', 'to', 'meet', 'Van', 'RossumðŸ˜€']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Van',\n",
       " 'Rossum',\n",
       " 'is',\n",
       " 'Python',\n",
       " 'Creator',\n",
       " 'ðŸ˜‰',\n",
       " ',',\n",
       " 'visiting',\n",
       " 'Pune',\n",
       " 'this',\n",
       " 'week',\n",
       " 'ðŸ˜',\n",
       " '.',\n",
       " 'The',\n",
       " 'development',\n",
       " 'community',\n",
       " 'is',\n",
       " 'very',\n",
       " 'eager',\n",
       " 'to',\n",
       " 'meet',\n",
       " 'Van',\n",
       " 'Rossum',\n",
       " 'ðŸ˜€']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1 = '''The Van Rossum is Python CreatorðŸ˜‰, visiting Pune this week ðŸ˜. The\n",
    "development community is very eager to meet Van RossumðŸ˜€'''\n",
    "\n",
    "# TweetTokenizer(): Tokenizer for tweets.\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Creating the Object\n",
    "tk = TweetTokenizer()\n",
    "\n",
    "print(sent1)\n",
    "\n",
    "print(word_tokenize(sent1))\n",
    "\n",
    "# Tokenize the data - Tokenization\n",
    "tk.tokenize(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      "This\n",
      "is\n",
      "some\n",
      "text\n",
      "with\n",
      "punctuation\n",
      ">\n",
      "Let's\n",
      "tokenize\n",
      "it\n",
      "Is\n",
      "it\n",
      "ok\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def custom_tokenizer(text):\n",
    "    return re.split(r\"[.,;?!\\s]+\", text)\n",
    "\n",
    "text = \"This is some text with punctuation > Let's tokenize it. Is it ok?\"\n",
    "\n",
    "tokens = custom_tokenizer(text)\n",
    "\n",
    "print(\"Tokens:\")\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('student3.tsv')\n",
    "data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roll\tname\tclass\tmarks\tage\n",
      "1\tanil\tTE\t56.77\t22\n",
      "2\tamit\tTE\t59.77\t21\n",
      "3\taniket\tBE\t76.88\t19\n",
      "4\tajinkya\tTE\t69.66\t20\n",
      "5\tasha\tTE\t63.28\t20\n",
      "6\tayesha\tBE\t49.55\t20\n",
      "7\tamar\tBE\t65.34\t19\n",
      "8\tamita\tBE\t68.33\t23\n",
      "9\tamol\tTE\t56.75\t20\n",
      "10\tanmol\tBE\t78.66\t21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 'anil', 'TE', 56.77, 22], [2, 'amit', 'TE', 59.77, 21], [3, 'aniket', 'BE', 76.88, 19], [4, 'ajinkya', 'TE', 69.66, 20], [5, 'asha', 'TE', 63.28, 20], [6, 'ayesha', 'BE', 49.55, 20], [7, 'amar', 'BE', 65.34, 19], [8, 'amita', 'BE', 68.33, 23], [9, 'amol', 'TE', 56.75, 20], [10, 'anmol', 'BE', 78.66, 21]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import LineTokenizer\n",
    "\n",
    "# Creating the Object\n",
    "tk = LineTokenizer()\n",
    "\n",
    "# Tokenize the data - Tokenization\n",
    "tk.tokenize(data)\n",
    "final=[]\n",
    "for i in tk.tokenize(data):\n",
    "    a=[]\n",
    "    row = i.split('\\t')\n",
    "    for word in row:\n",
    "        if word.isalpha()==False:\n",
    "            a.append(eval(word))\n",
    "        else:\n",
    "            a.append(word)\n",
    "    final.append(a)\n",
    "\n",
    "final.pop(0)\n",
    "print(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
