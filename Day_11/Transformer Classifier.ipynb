{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Administrator.DAI-PC2\\anaconda3\\envs\\DNN_ENV\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.datasets import imdb\n",
    "from keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definer the Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(Layer):\n",
    "\n",
    "    def __init__(self,embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        # embed_dim: Dimensionaloty of I/P & O/P\n",
    "        # num_head = Number of attention heads\n",
    "        # ff_dim: dimensionality of Feed Forward\n",
    "        # rate: dropout rate\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Creation of Multi Head Attention layer, responsible for learning long range\n",
    "        self.att = MultiHeadAttention(num_heads, key_dim = embed_dim) # Here, Attention Score is calculated\n",
    "\n",
    "        # Self.fnn: Creation of Feed-Forward Neural Network, often used for additional normalization\n",
    "        self.ffn = Sequential(\n",
    "            [Dense(ff_dim, activation='relu'), Dense(embed_dim),]\n",
    "        )\n",
    "        \n",
    "        # self.layernorm: Layer normalization\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        # self.dropout: Dropout rates\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        \n",
    "        # Applies Multi Head Attention to input sequence allowing different part\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "\n",
    "        # Applies Dropout to the attention output\n",
    "        attn_output = self.dropout1(attn_output, training = training)\n",
    "\n",
    "        # Adds the attention output to original inpput and applies layer normalization\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "\n",
    "        # Passes the normalized output through Feed-Forward Network\n",
    "        ffn_output = self.ffn(out1)\n",
    "\n",
    "        # Applies dropout to Feed-Forward output\n",
    "        ffn_output = self.dropout2(ffn_output, training = training)\n",
    "\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(Layer):\n",
    "\n",
    "    def __init__(self,maxlen, vocab_size, embed_dim):\n",
    "        # maxlen: Max. length of input sequences\n",
    "        # vocab_size: Total no. of unique tokens (words) in vocab.\n",
    "        # embed_dim: dimensionality of embedding\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding layer that maps each token in input sequence to a dense vector of size embed_dim\n",
    "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "\n",
    "        # Embedding layer for mapping in sequence for each postion (from 0 to maxlen - 1) to a dense vector of size embed_dim\n",
    "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Extracts the actual length of current input sequence\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "\n",
    "        # Create a tensor of positions from 0 to maxlen - 1\n",
    "        positions = tf.range(start = 0, limit=maxlen, delta=1)\n",
    "\n",
    "        # Looks up the position embeddings for each position in the sequence\n",
    "        positions = self.pos_emb(positions)\n",
    "\n",
    "        # Looks up the token embeddings for each token in input sequence\n",
    "        x = self.token_emb(x)\n",
    "\n",
    "        # Adds the token and positions embeddings element-wise, resulting in a combined representation that captures both word meaning and psoitional information. \n",
    "        return x + positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000 # Considering the Top-20000 words\n",
    "maxlen = 200 # Considering 1st 200 words of each movie review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 Training sequence\n",
      "25000 Validation sequence\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_val, y_val) = imdb.load_data(num_words=vocab_size)\n",
    "print(len(X_train), 'Training sequence')\n",
    "print(len(X_val), 'Validation sequence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen = maxlen)\n",
    "X_val = pad_sequences(X_val, maxlen = maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 200)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   89,     8,   511,  6339,    59,  2013,    41,   523,   147,\n",
       "        1876,     5, 18733,   175,   347,    11,   618,     4,   172,\n",
       "          96,  2329,     2,     9,   862,  4722,     8,    41,     5,\n",
       "          27,   532,  2904,     9,  5750,     4,  9910,   136,  7900,\n",
       "        9287,     5,     2,    19,  1456,   921,    42,  2475,  1488,\n",
       "          68,  2456,   216,    17,     6,  2143,    48,    13,    69,\n",
       "           6, 12928,    13,    62,    28,  2564,    12,     8,    98,\n",
       "         634,   908,    10,    10,  2047,  3423,     9, 14790,    17,\n",
       "           2,     6,    87,  1465,    48,    25,   377,    27,   478,\n",
       "         157,    11,     2, 18497,    29,  2010,     4,  2915,     7,\n",
       "        5712, 12710,    83,     6,  3207,     2,     7,   107,    42,\n",
       "         289,   715,   257,     5,    95,  9727,     4, 13331,    11,\n",
       "          17, 10846,     5, 13869,  1377,    17,   614,    11,    14,\n",
       "         365,  1652,     2,     2,   373,    10,    10,     4,   167,\n",
       "        6184,     2,   287,    64,    35,     2,  3470,     7,  1489,\n",
       "           4,   370,   121,    12,    80,   123,   178,    51,    75,\n",
       "         181,     8,    67,     4,   636, 10227,     9,  3735,  3316,\n",
       "         190,    50,     9,   486,    54,    11,     6,   303,   548,\n",
       "        6548,   684,  8135,     2,   208,    11,     4,     2,     2,\n",
       "          95,  5115,     4,  4154,  5425,   190,   122,    15,    79,\n",
       "         143,    10,    10,  1479,  1468,     9,     6,   196,   297,\n",
       "          14,   310,     9,    24,  1178,    18, 10552,   361,    42,\n",
       "          76,   334])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 32\n",
    "num_heads =  2\n",
    "ff_dim = 32\n",
    "\n",
    "inputs = Input(shape=(maxlen,))\n",
    "\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "X = embedding_layer(inputs)\n",
    "\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "X = transformer_block(X)\n",
    "\n",
    "X = GlobalAveragePooling1D()(X)\n",
    "X = Dropout(0.1)(X)\n",
    "X = Dense(20, activation ='relu')(X)\n",
    "X = Dropout(0.1)(X)\n",
    "\n",
    "outputs = Dense(2, activation = 'softmax')(X)\n",
    "\n",
    "model = Model(inputs=inputs, outputs = outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_10 (InputLayer)       [(None, 200)]             0         \n",
      "                                                                 \n",
      " token_and_position_embeddi  (None, 200, 32)           646400    \n",
      " ng_9 (TokenAndPositionEmbe                                      \n",
      " dding)                                                          \n",
      "                                                                 \n",
      " transformer_block_9 (Trans  (None, 200, 32)           10656     \n",
      " formerBlock)                                                    \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 32)                0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 20)                660       \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 20)                0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 2)                 42        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 657758 (2.51 MB)\n",
      "Trainable params: 657758 (2.51 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Administrator.DAI-PC2\\anaconda3\\envs\\DNN_ENV\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = 'adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\Administrator.DAI-PC2\\anaconda3\\envs\\DNN_ENV\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Administrator.DAI-PC2\\anaconda3\\envs\\DNN_ENV\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "782/782 [==============================] - 41s 45ms/step - loss: 0.3862 - accuracy: 0.8147 - val_loss: 0.3107 - val_accuracy: 0.8606\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 34s 44ms/step - loss: 0.1982 - accuracy: 0.9222 - val_loss: 0.3150 - val_accuracy: 0.8720\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 34s 43ms/step - loss: 0.1273 - accuracy: 0.9546 - val_loss: 0.3631 - val_accuracy: 0.8617\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 34s 43ms/step - loss: 0.0919 - accuracy: 0.9684 - val_loss: 0.4710 - val_accuracy: 0.8399\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 34s 44ms/step - loss: 0.0627 - accuracy: 0.9798 - val_loss: 0.6030 - val_accuracy: 0.8457\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 33s 42ms/step - loss: 0.0453 - accuracy: 0.9859 - val_loss: 0.6484 - val_accuracy: 0.8410\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 34s 44ms/step - loss: 0.0324 - accuracy: 0.9904 - val_loss: 0.7577 - val_accuracy: 0.8418\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 34s 44ms/step - loss: 0.0283 - accuracy: 0.9913 - val_loss: 0.7884 - val_accuracy: 0.8376\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 33s 42ms/step - loss: 0.0194 - accuracy: 0.9943 - val_loss: 0.9525 - val_accuracy: 0.8384\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 33s 42ms/step - loss: 0.0189 - accuracy: 0.9944 - val_loss: 0.9369 - val_accuracy: 0.8302\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 32, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DNN_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
