{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "import re\n",
    "url = urlopen('https://en.wikipedia.org/wiki/Sri_Lanka')\n",
    "data = url.read()\n",
    "\n",
    "soup = BeautifulSoup(data, 'html.parser')\n",
    "\n",
    "text=''\n",
    "\n",
    "limited_tags = soup.find_all('p')\n",
    " \n",
    "for tag in limited_tags:\n",
    "    text=text+tag.text+' '\n",
    "text = re.sub(\"\\[[0-9]+\\]\", '', text)\n",
    "text = re.sub(\"\\[[a-zA-Z]+\\]\", '', text)\n",
    "print(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_parser =PlaintextParser.from_string(text,Tokenizer('english'))\n",
    "text_rank_summarizer=TextRankSummarizer()\n",
    "summary=text_rank_summarizer(my_parser.document,sentences_count=10)\n",
    "summary\n",
    "for sent in summary:\n",
    "    print(sent,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lex_rank_summarizer=LexRankSummarizer()\n",
    "summary=lex_rank_summarizer(my_parser.document,sentences_count=10)\n",
    "summary\n",
    "for sent in summary:\n",
    "    print(sent,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lsa_summarizer=LsaSummarizer()\n",
    "summary=lsa_summarizer(my_parser.document,sentences_count=10)\n",
    "summary\n",
    "for sent in summary:\n",
    "    print(sent,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "url = urlopen('https://en.wikipedia.org/wiki/Sachin_Tendulkar')\n",
    "data = url.read()\n",
    "\n",
    "soup = BeautifulSoup(data, 'html.parser')\n",
    "text=''\n",
    "\n",
    "limited_tags = soup.find_all('p')\n",
    " \n",
    "for tag in limited_tags:\n",
    "    text=text+tag.text+' '\n",
    "text = re.sub(\"\\[[0-9]+\\]\", '', text)\n",
    "text = re.sub(\"\\[[a-zA-Z]+\\]\", '', text)\n",
    "\n",
    "text_summarizer=pipeline(\"summarization\")\n",
    "output= text_summarizer(text,min_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[1. \"\"\n",
    "\"The answer to the problem lies in the key details.\"\n",
    "2. \n",
    "3. \"The knife has a very sharp blade.\"\n",
    "\"His mind was always sharp and quick.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mechanical device used to wind another device that is driven by a spring (as a clock)\n",
      "vandalize a car by scratching the sides with a key\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "from nltk.tokenize import word_tokenize\n",
    "a1 = lesk(word_tokenize('He used the key to unlock the door.'),'key')\n",
    "print(a1.definition())\n",
    "a2 = lesk(word_tokenize(\"The answer to the problem lies in the key details.\"), 'key')\n",
    "print(a2.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = lesk(word_tokenize('He used the key to unlock the door.'),'key')\n",
    "print(a1.definition())\n",
    "a2 = lesk(word_tokenize(\"The answer to the problem lies in the key details.\"), 'key')\n",
    "print(a2.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size or measure according to a scale\n",
      "size or measure according to a scale\n"
     ]
    }
   ],
   "source": [
    "a1 = lesk(word_tokenize('The weight is measured on a scale weighing instrument.'),'scale')\n",
    "print(a1.definition())\n",
    "a2 = lesk(word_tokenize(\"The project is too large in scale for one person.\"), 'scale')\n",
    "print(a2.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "having or emitting a high-pitched and sharp tone or tones\n",
      "having or emitting a high-pitched and sharp tone or tones\n"
     ]
    }
   ],
   "source": [
    "a1 = lesk(word_tokenize('\"The knife has a very sharp blade.\"'),'sharp')\n",
    "print(a1.definition())\n",
    "a2 = lesk(word_tokenize(\"His mind was always sharp and quick.\"), 'sharp')\n",
    "print(a2.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>arrived</th>\n",
       "      <th>beautiful</th>\n",
       "      <th>before</th>\n",
       "      <th>birthday</th>\n",
       "      <th>check</th>\n",
       "      <th>delivered</th>\n",
       "      <th>delivery</th>\n",
       "      <th>due</th>\n",
       "      <th>fast</th>\n",
       "      <th>...</th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "      <th>traffic</th>\n",
       "      <th>truck</th>\n",
       "      <th>variety</th>\n",
       "      <th>we</th>\n",
       "      <th>wide</th>\n",
       "      <th>with</th>\n",
       "      <th>wrapped</th>\n",
       "      <th>wrong</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   address  arrived  beautiful  before  birthday  check  delivered  delivery  \\\n",
       "1        1        0          0       0         0      0          1         0   \n",
       "2        0        0          1       0         1      0          0         0   \n",
       "3        0        1          0       0         0      0          0         1   \n",
       "4        1        0          0       1         0      1          0         0   \n",
       "5        0        0          0       0         0      0          0         1   \n",
       "\n",
       "   due  fast  ...  the  to  traffic  truck  variety  we  wide  with  wrapped  \\\n",
       "1    0     0  ...    3   1        0      0        0   0     0     0        0   \n",
       "2    0     0  ...    0   0        0      0        0   0     0     0        1   \n",
       "3    1     0  ...    1   1        1      1        0   0     0     0        0   \n",
       "4    0     0  ...    2   1        0      0        0   1     0     0        0   \n",
       "5    0     1  ...    0   0        0      0        1   0     1     1        0   \n",
       "\n",
       "   wrong  \n",
       "1      1  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  \n",
       "5      0  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "s1 = \"The postman delivered the package to the wrong address.\"\n",
    "s2 = \"I wrapped a beautiful present for my friend's birthday.\"\n",
    "s3=\"The delivery truck arrived late due to heavy traffic.\"\n",
    "s4=\"We need to check the shipping address before sending the order.\"\n",
    "s5=\"Online shopping offers a wide variety of products with fast delivery.\"\n",
    "tokens = word_tokenize(s1.lower()+s2.lower()+s3.lower()+s4.lower()+s5.lower())\n",
    "tokens=set(tokens)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(index=[1,2,3,4,5], columns=list(tokens))\n",
    "\n",
    "tokens1 = word_tokenize(s1.lower())\n",
    "tokens2 = word_tokenize(s2.lower())\n",
    "tokens3 = word_tokenize(s3.lower())\n",
    "tokens4 = word_tokenize(s4.lower())\n",
    "tokens5 = word_tokenize(s5.lower())\n",
    "\n",
    "count1 = [tokens1.count(x) for x in df.columns]\n",
    "count2 = [tokens2.count(x) for x in df.columns]\n",
    "count3 = [tokens3.count(x) for x in df.columns]\n",
    "count4 = [tokens4.count(x) for x in df.columns]\n",
    "count5 = [tokens5.count(x) for x in df.columns]\n",
    "cvt = CountVectorizer()\n",
    "new_data = cvt.fit_transform((s1,s2,s3,s4,s5))\n",
    "df = pd.DataFrame(index=[1,2,3,4,5], columns=cvt.get_feature_names_out(), data=new_data.toarray())\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
